{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python WebScrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of webscraping,I made use of a python library called praw which makes the process of web scrapping as easy as a single function call.An alternative could have been selenium+beautiful-soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Using cached https://files.pythonhosted.org/packages/25/c0/b9714b4fb164368843b41482a3cac11938021871adf99bf5aaa3980b0182/praw-6.5.1-py3-none-any.whl\n",
      "Collecting prawcore<2.0,>=1.0.1 (from praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/c9/8e/d076cb8f26523f91eef3e75d6cf9143b2f16d67ce7d681a61d0bbc783f49/prawcore-1.3.0-py3-none-any.whl\n",
      "Collecting websocket-client>=0.54.0 (from praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl\n",
      "Collecting update-checker>=0.16 (from praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
      "Collecting requests<3.0,>=2.6.0 (from prawcore<2.0,>=1.0.1->praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\n",
      "Collecting six (from websocket-client>=0.54.0->praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/57/2b/26e37a4b034800c960a00c4e1b3d9ca5d7014e983e6e729e33ea2f36426c/certifi-2020.4.5.1-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\n",
      "Installing collected packages: chardet, idna, certifi, urllib3, requests, prawcore, six, websocket-client, update-checker, praw\n",
      "Successfully installed certifi-2020.4.5.1 chardet-3.0.4 idna-2.9 praw-6.5.1 prawcore-1.3.0 requests-2.23.0 six-1.14.0 update-checker-0.16 urllib3-1.25.9 websocket-client-0.57.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from praw.models import MoreComments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='HcjyI-VCKlu3pQ',client_secret = '5igSrCH2dvgW3VZ8XbRIJr4ZQsI',user_agent='scrape_reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this block scrapes the data in reddits' hot category.\n",
    "\n",
    "s = []\n",
    "final_s = []\n",
    "def hot_scrape():\n",
    "    get_data = reddit.subreddit('india').hot(limit=None)\n",
    "    hot_posts = []\n",
    "    for data in get_data:\n",
    "        comment = []\n",
    "        num_com = 10\n",
    "        l = len(data.comments)\n",
    "        if l > 0:\n",
    "            if l < 10:\n",
    "                num_com = l\n",
    "            r = np.random.choice(l, num_com, replace=False) \n",
    "            for i in r:\n",
    "                if isinstance(data.comments[i], MoreComments):\n",
    "                    continue\n",
    "                else:\n",
    "                    comment.append(data.comments[i].body)\n",
    "        if l<0:\n",
    "            continue\n",
    "        hot_posts.append([data.title, data.score, data.id, data.subreddit, data.url, data.num_comments, data.selftext, data.created,comment,data.link_flair_text])\n",
    "    hot_posts = pd.DataFrame(hot_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','comments','flair'])        \n",
    "    return hot_posts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this block scrapes the data in reddits' top category.\n",
    "\n",
    "def top_scrape():\n",
    "    top_data = reddit.subreddit('india').top(limit=None)\n",
    "    top_posts = []\n",
    "    for data in top_data:\n",
    "        comment = []\n",
    "        num_com = 10\n",
    "        l = len(data.comments)\n",
    "        if l > 0:\n",
    "            if l < 10:\n",
    "                num_com = l\n",
    "        r = np.random.choice(l, num_com, replace=False) \n",
    "        for i in r:\n",
    "            if isinstance(data.comments[i], MoreComments):\n",
    "                continue\n",
    "            else:\n",
    "                comment.append(data.comments[i].body)\n",
    "        top_posts.append([data.title, data.score, data.id, data.subreddit, data.url, data.num_comments, data.selftext, data.created,comment,data.link_flair_text])\n",
    "    top_posts = pd.DataFrame(top_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','comments','flair'])\n",
    "    top_posts['body'].replace('', np.nan, inplace=True)\n",
    "    return top_posts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this block scrapes the data in reddits' new category.\n",
    "def new_scrape():\n",
    "    new_data = reddit.subreddit('india').new(limit=None)\n",
    "    new_posts = []\n",
    "    for data in new_data:\n",
    "        comment = []\n",
    "        num_com = 10\n",
    "        l = len(data.comments)\n",
    "        if l > 0:\n",
    "            if l < 10:\n",
    "                num_com = l\n",
    "            r = np.random.choice(l, num_com, replace=False) \n",
    "            for i in r:\n",
    "                if isinstance(data.comments[i], MoreComments):\n",
    "                    continue\n",
    "                else:\n",
    "                    comment.append(data.comments[i].body)\n",
    "        if l < 0:\n",
    "            continue\n",
    "\n",
    "        new_posts.append([data.title, data.score, data.id, data.subreddit, data.url, data.num_comments, data.selftext, data.created,comment,data.link_flair_text])\n",
    "    new_posts = pd.DataFrame(new_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','comments','flair'])\n",
    "    new_posts['body'].replace('', np.nan, inplace=True)\n",
    "    return new_posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = new_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = top_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = hot_scrape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = a.shape[0]+ b.shape[0]+ c.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_top_new_posts = pd.concat([a,b,c],ignore_index=True)\n",
    "hot_top_new_posts.drop_duplicates(subset =\"id\",keep = 'first', inplace = True)\n",
    "hot_top_new_posts.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_top_new_posts.to_csv('data_new.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
